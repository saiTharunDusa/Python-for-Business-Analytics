# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E81kyArcuIAt6v89fsB77fH6X1GjjjnV
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import seaborn as sns

# read the csv file.
airlines_subset_df = pd.read_csv('/content/airlines_subset.csv')
print(airlines_subset_df.head())
print(airlines_subset_df.describe())
print(airlines_subset_df.info())
print()

"""1. Based on the descriptive statistics, the mean of the delayed column is 0.46.
2. This indicates that there less no of delayed flights. And to be precise, 46% of the flights were delayed.
"""

# Logistic regression to predict the probability of delayed flights.
model = LogisticRegression(solver="liblinear", random_state=0)
x = airlines_subset_df['flight_length'].values.reshape(-1,1)
y = airlines_subset_df['delayed'].values.reshape(-1,1)
model.fit(x, y)

# Generate the predicted probablities.
airlines_subset_df['logistic_regression'] = model.predict_proba(x)[:,1]

# print the header for logistic regression
print(airlines_subset_df.head())
print()

# 10 largest predicted probabilities for delayed flights
print('10 largest predicted probabilities for delayed flights:')
print(airlines_subset_df.nlargest(10, 'logistic_regression'))
print()

"""Interpretation and Comments:
1. Based on the 10 largest predicted probabilities for delayed flights, the flight num 1007 have high chance of getting delayed.
2. By observing the delayed column, we could see 8 true delays and 2 false delays out of 10.
3. This suggests that model is confident in predicting the probablity of flights that are to be delayed.

"""

# create a confusion matrix
the_median = airlines_subset_df['logistic_regression'].median()
# create a list with value 1 if the median values are above median.
prediction = list(1 * airlines_subset_df['logistic_regression'] > the_median)
# create a list with value 0 if the median values are below the median value.
actual = list(airlines_subset_df['delayed'])
print('Confusion Matrix: ')
print('[[true positives] [false positives]]')
print('[[false negatives] [true negatives]]')
conf_mat = confusion_matrix(prediction, actual)
print()
print(conf_mat)

# calculate the accuracy scores based on the generated confusion matrix.
# Precision - How many actually positive out of what we thought would be positive
# Recall (sensitivity) - From the true positive cases, how many did we think were positive

print()
precision = conf_mat[0][0] /(conf_mat[0][0] + conf_mat[0][1])
print(f"Precision: {precision}")
print()
recall = conf_mat[0][0] /(conf_mat[0][0] + conf_mat[1][0])
print(f"Recall : {recall}")
print()

"""Interpretation and Comments:

1. True Positives: This means how many model could predict correctly. And there are 156.

2. False Positives: This means how many model predicted got wrong. And there are 114.

3. False Negatives: This means model predicted some flights wont get delayed, but they were actually delayed. And there are 105.

4. True Negatives: This mean model predicted some flights would get delayed, and they are actually delayed. And there are 110.

Precision: With a precision value of about 0.578, it means that out of all the instances the model predicted as positive, around 57.8% of them are actually true positives. This shows that the model is quite cautious in its positive predictions, aiming to avoid false positives.

Recall: The recall value of approximately 0.598 indicates that the model can identify around 59.8% of all the actual positive instances. This metric demonstrates the model's sensitivity in capturing true positives from the entire pool of positive cases, highlighting its ability to detect positive instances accurately.
"""

